# SageAttention Builder - Optimized multi-stage build for SageAttention wheels
# Supports Linux with multiple PyTorch versions using shared dependencies

# Declare build arguments that are used in FROM statements
ARG CUDA_VERSION
ARG PYTHON_VERSION=3.12
ARG TORCH_VERSION
ARG TORCH_CUDA_ARCH_LIST

# Stage 1: Base environment with system dependencies
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-devel-ubuntu24.04 AS base-environment

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=$CUDA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
ENV PYTHONUNBUFFERED=1

# Install system dependencies (cached layer)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-dev \
    python3-venv \
    python3-pip \
    build-essential \
    git \
    wget \
    curl \
    ca-certificates \
    && apt-get autoremove -y \
    && apt-get autoclean \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* \
    && rm -rf /var/tmp/*

# Create virtual environment (cached layer)
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip and install build tools (cached layer)
RUN pip install --upgrade pip setuptools wheel packaging

# Stage 2: PyTorch dependency layer
FROM base-environment AS pytorch-environment

# Re-declare ARGs after FROM
ARG CUDA_VERSION
ARG TORCH_VERSION

# Set environment variables
ENV TORCH_VERSION=${TORCH_VERSION}
ENV CUDA_VERSION=${CUDA_VERSION}

# Install PyTorch with caching optimization
RUN --mount=type=cache,target=/root/.cache/pip \
    if [ "$CUDA_VERSION" = "12.8.1" ]; then \
        pip install torch==${TORCH_VERSION} torchvision --index-url https://download.pytorch.org/whl/cu128; \
    else \
        pip install torch==${TORCH_VERSION} torchvision --index-url https://download.pytorch.org/whl/cu129; \
    fi

# Stage 3: SageAttention builder
FROM pytorch-environment AS sageattention-builder

# Re-declare all ARGs for this stage
ARG TORCH_VERSION
ARG PYTHON_VERSION
ARG TORCH_CUDA_ARCH_LIST

# Set remaining environment variables
ENV TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}
ENV PYTHON_VERSION=${PYTHON_VERSION}

# Verify ARG values
RUN echo "TORCH_VERSION: ${TORCH_VERSION}" && \
    echo "PYTHON_VERSION: ${PYTHON_VERSION}" && \
    echo "CUDA_VERSION: ${CUDA_VERSION}" && \
    echo "TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST}"

# Copy dependency files first for better caching
WORKDIR /build
COPY pyproject.toml setup.py update_pyproject.py simpleindex.toml ./

# Update pyproject.toml with correct PyTorch version
RUN python update_pyproject.py

# Install SageAttention build dependencies (cached if dependencies unchanged)
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -e .

# Copy the local SageAttention source code (invalidates cache only when source changes)
COPY . /build/SageAttention
WORKDIR /build/SageAttention

# Build SageAttention wheel
RUN python setup.py bdist_wheel

# Stage 4: Wheel extraction (minimal, just copy the wheel)
FROM pytorch-environment AS sageattention-wheel
WORKDIR /wheels
COPY --from=sageattention-builder /build/SageAttention/dist/*.whl ./
RUN ls -la && echo "Wheel extracted successfully"

# Stage 5: Runtime verification (reuses pytorch-environment)
FROM pytorch-environment AS sageattention-test

# Re-declare ARGs for test stage
ARG TORCH_VERSION
ARG PYTHON_VERSION
ARG CUDA_VERSION

# Set environment variables
ENV TORCH_VERSION=${TORCH_VERSION}
ENV PYTHON_VERSION=${PYTHON_VERSION}
ENV CUDA_VERSION=${CUDA_VERSION}

# Copy and install the built wheel (PyTorch already installed from base)
COPY --from=sageattention-wheel /wheels/*.whl /tmp/
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install /tmp/*.whl \
    && rm -rf /tmp/*

# Test SageAttention import
RUN python -c "import sageattention; print('SageAttention imported successfully')"